# -*- coding: utf-8 -*-
"""Binary_Classification_Amol_Rakhunde.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PxS4455EilId0MOjf9tsgZrNT84o1kPI

# **Task**

**Binary Classification:**
1. Exploratory analysis of the dataset provided
2. Feature Selection
3. Preprocessing 
2. Training a model to classify as class ‘0’ or class ‘1’.

# **Importing Libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing basic required libraries
import pandas as pd
import numpy as np

# Importing librarires for visualization
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

# Importing libraries required for building models
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import roc_auc_score,classification_report,confusion_matrix,log_loss, accuracy_score, recall_score, precision_score, f1_score

import warnings
warnings.filterwarnings("ignore")

# Loading training data set
train_data = pd.read_csv('/content/drive/MyDrive/Company Assignment/Arya AI/training_set.csv')

# Setting maximum limist to show columns and rows
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)

"""# **Data Cleaning**"""

# Look for number of rows and columns in dataset
train_data.shape

# First look of data
train_data.head()

train_data.tail()

# Droping column 'Unnamed: 0' as we have index already
train_data.drop('Unnamed: 0', axis=1, inplace=True)

"""## Checking for duplicates:"""

# Number of duplicates present in dataset
len(train_data[train_data.duplicated()])

"""* 296 rows have duplicates."""

# Removing all the duplicates by keeping first row from duplicates
train_data.drop_duplicates(inplace=True)

# Check for removed duplicates
len(train_data[train_data.duplicated()])

"""* Now no rows are duplicated."""

# shape will change
train_data.shape

"""## Checking for Null Values:

"""

# sum of all null the values in dataset
train_data.isna().sum().sum()

"""* Don't have any null value.

## Summary of Data Types & Data:
"""

# to look for data types of all features
train_data.info()

"""* Data types of all the features are correct."""

# Summary of all the features
train_data.describe().T

"""* Most of the features contains zeros only.

## Sparsity of Data:
"""

a = (train_data.to_numpy() == 0.00).mean()*100
print (a)

"""* Almost 77% of data is zeros."""

# Percentage of sparsity for each column
for col in train_data.columns:
  total_zeros = train_data[train_data[col] == 0.00].shape[0]
  print(f"{col} : {round(total_zeros/train_data.shape[0]*100,2)}")

"""* 17 features contains more than 90% zeros only.

# **EDA**
"""

# Creating copy of data
df = train_data.copy()

"""## **Univariate Analysis:**"""

# Checking for class proprtion in dependent varibale
df['Y'].value_counts().plot.pie(autopct='%1.1f%%', shadow=True, figsize=(5,5))
plt.title('Pie Chart for Y')
plt.show()

"""* We have 40% - 60% class data.
* Dataset is slight imbalance.
"""

# Distribution of each feature
plt.figure(figsize =(20,50))
for i , variable in enumerate(df.columns[:-1]):
    plt.subplot(17, 4, i +1)
    sns.distplot(x = df[variable])
    plt.tight_layout()
    plt.title(variable)
    
plt.show()

"""* All features are highly right skewed.
* Which means that outliers are present in data.
"""

# Plotting box plot for outlier detection
plt.figure(figsize =(20,50))
for i , variable in enumerate(df.columns[:-1]):
    plt.subplot(17, 4, i +1)
    sns.boxplot(x = df[variable])
    plt.tight_layout()
    plt.title(variable)
    
plt.show()

"""* Every feature has some outliers.
* Dataset contains only 3614 rows, that is dataset is not huge.
* So it wont be feasible to remove all the outliers.
* So we will be doing standardisation, which can solve problem of outliers to some extent.

## **Standardization**
"""

# Seperating dependent and independent varibales
X = df.drop(['Y'],axis=1) # independent varibale
y = df['Y'] # dependent varibale

# standardisation; fitting and transorming simulteneously
scaler = StandardScaler()
X = scaler.fit_transform(X)

# standardized values arrays
X

# Converting those arrays into dataframe
X = pd.DataFrame(X,columns=train_data.columns[:-1])

X

# looking for outliers after standardization
plt.figure(figsize =(20,50))
for i , variable in enumerate(X.columns[:-1]):
    plt.subplot(17, 4, i +1)
    sns.boxplot(x = X[variable])
    plt.tight_layout()
    plt.title(variable)
    
plt.show()

"""* Standardization reduced the some of the outliers.
* But still outliers are present in dataset.
* So, will be using models and techniques which robust to outliers.

# **Feature Selection**

## **PCA for Feature Selection**

* Data contain outliers.
* PCA is sensitive to outliers.
* Data contains only 3766 rows, removing outliers will reduce data.

## **Random Forest Classifer for Feature Selection**

### Splitting Data
"""

# Splitting data into training and validation set in 4:1 ratio.(80%:20%)
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

X_train.shape

X_val.shape

# percentage of class in training dataset
y_train.value_counts()/y_train.shape[0]*100

# percentage of class in validation dataset
y_val.value_counts()/y_val.shape[0]*100

"""* For training and validation dataset, class proprtion is almost same."""

# creating intance of Random Forest Classifier
rfs = RandomForestClassifier()
# fitting model
rfs.fit(X_train,y_train)

# getting importance values for each feature
feature_importance = rfs.feature_importances_

feature_importance

# Converting above feature importance array into datframe
features_df = pd.DataFrame(feature_importance, index=X.columns)

# sorting dataframe in decreasing order of feature importance
features_df.sort_values(0, ascending = False, inplace = True)

# giving index to dataframe
features_df.reset_index(inplace=True)

# Changing column name
features_df.set_axis(['feature', 'importance'], axis=1, inplace=True)

# Cumulative feature importance plot and thresold line at 0.9
plt.figure(figsize =(6,5))
features_df['importance'].cumsum().plot()
plt.hlines(y=0.9, xmin=0, xmax=50, colors='r', linestyles='--', lw=2, label='90% Cummulative Importance Line')
plt.legend(bbox_to_anchor=(1.04,0.5), loc="center left", borderaxespad=0)
plt.xlabel('Number of Features')
plt.ylabel('Cummulative Featture Importance')
plt.show()

# getting number of features where cumulative feature importance is equal to 0.9
features_df['importance'].cumsum().div(features_df['importance'].sum()).le(0.90).sum()

# selecting first 24 top features
final_df = features_df[0:24]

# bar plot for top features
plt.figure(figsize=(12,5))
sns.barplot(y='importance', x = 'feature', data = final_df)
plt.title('Feature Importance of Top 24 Features')
plt.xlabel('Features')
plt.ylabel('Feature Importance')
plt.show()

final_df

# list of top 24 features 
top_features = list(final_df['feature'])

# training dataset with top 24 features
X_train_final = X_train[top_features]

# validation dataset with top 24 features
X_val_final = X_val[top_features]

"""# **Model Making**

* Data contains outliers.
* Classification models such as Logistic Regressio, KNN and bossting algorithms like gradient boosting and XGboost are sensitive to outliers.
* But Decision Tree models and Random Forest Classifier are robust to outliers.
* So, Decision Tree and Random Forest Classifiers will be used to build a model.

## **Decision Tree Classifier:**

### **Base Model**
"""

# Using Decision Tree Classifier
classifier = DecisionTreeClassifier(random_state=42)          
dtc = classifier.fit(X_train_final, y_train)

# class prediction
y_val_pred_dtc = dtc.predict(X_val_final)                   # for validation set
y_train_pred_dtc = dtc.predict(X_train_final)               # for training set

train_accuracy_dtc = round(accuracy_score(y_train_pred_dtc,y_train), 3)
val_accuracy_dtc = round(accuracy_score(y_val_pred_dtc,y_val), 3)
roc_score_dtc = round(roc_auc_score(y_val_pred_dtc,y_val), 3)

print("The accuracy on train data is ", train_accuracy_dtc)
print("The accuracy on validation data is ", val_accuracy_dtc)
print("The roc_score on validation data is ", roc_score_dtc)

"""* Difeerence between train and validation accuracy show overfitting.
* So performed cross validation and hyperparameter tunning to reduce overfitting.

### **Hyperparameter Tuning on Decision Tree Classifier:**
"""

# The maximum depth of the tree
depth_of_tree = [15,20,25,30]

# The minimum number of samples required to split an internal node
min_samples_split = [0.001,0.01,0.05]

# Minimum number of samples required at each leaf node
min_samples_leaf = [10,20,30,40]

# Hyperparameter Grid
param_dict = {'max_depth': depth_of_tree,
              'min_samples_split': min_samples_split,
              'min_samples_leaf': min_samples_leaf}

# Grid search
dtc_grid = GridSearchCV(estimator = DecisionTreeClassifier(),
                       param_grid = param_dict,
                       cv = 5, verbose=3, n_jobs = -1, scoring="neg_log_loss")
# fitting model
dtc_grid.fit(X_train_final, y_train)

dtc_grid.best_estimator_

dtc_grid.best_params_

# Creating model with best estimators
dtc_optimal_model = dtc_grid.best_estimator_

#class prediction of y on train and test
y_val_pred_dtc_grid=dtc_optimal_model.predict(X_val_final)
y_train_pred_dtc_grid=dtc_optimal_model.predict(X_train_final)

#getting all scores for decision tree after CV and Hyperparameter Tunning
train_accuracy_dtc_grid = round(accuracy_score(y_train_pred_dtc_grid,y_train), 3)
val_accuracy_dtc_grid = round(accuracy_score(y_val_pred_dtc_grid,y_val), 3)
auc_dtc_grid = round(roc_auc_score(y_val_pred_dtc_grid,y_val), 3)

print("The accuracy on train data is ", train_accuracy_dtc_grid)
print("The accuracy on validation data is ", val_accuracy_dtc_grid)
print("The auc on validation data is ", auc_dtc_grid)

"""* Accuracy on train and validation almost same, that is no overfitting.

## **Random Forest Classifier:**

## **Base Model**
"""

#fitting data into Random Forest Classifier
rf_classifier=RandomForestClassifier()
rfc = rf_classifier.fit(X_train_final, y_train)

#class prediction of y
y_val_pred_rfc=rfc.predict(X_val_final)
y_train_pred_rfc=rfc.predict(X_train_final)

#getting all scores for Random Forest Classifier
train_accuracy_rfc = round(accuracy_score(y_train_pred_rfc,y_train), 3)
val_accuracy_rfc = round(accuracy_score(y_val_pred_rfc,y_val), 3)
roc_score_rfc = round(roc_auc_score(y_val_pred_rfc,y_val), 3)

print("The accuracy on train data is ", train_accuracy_rfc)
print("The accuracy on validation data is ", val_accuracy_rfc)
print("The roc_score on validation data is ", roc_score_rfc)

"""* Difeerence between train and validation accuracy show overfitting.
* So performed cross validation and hyperparameter tunning to reduce overfitting.

## **Hyperparameter Tunning on Random Forest Classifier:**
"""

# Number of trees
n_estimators = [150,200,300]

# Maximum depth of trees
max_depth = [5,10,15]

# Minimum number of samples required to split a node
min_samples_split = [20,30,40,50]

# Minimum number of samples required at each leaf node
min_samples_leaf = [40,50]

# Hyperparameter Grid
param_dict = {'n_estimators' : n_estimators,
              'max_depth' : max_depth,
              'min_samples_split' : min_samples_split,
              'min_samples_leaf' : min_samples_leaf}

# Grid search
rfc_grid = GridSearchCV(estimator=RandomForestClassifier(),
                       param_grid = param_dict, cv=2,
                       verbose=0, scoring="neg_log_loss")
# fitting model
rfc_grid.fit(X_train_final,y_train)

rfc_grid.best_estimator_

rfc_grid.best_params_

# built a model using best estimators
rfc_optimal_model = rfc_grid.best_estimator_

#class prediction of y on train and test
y_val_pred_rfc_grid=rfc_optimal_model.predict(X_val_final)
y_train_pred_rfc_grid=rfc_optimal_model.predict(X_train_final)

#getting all scores for Random Forest Classifier after CV and Hyperparameter Tunning
train_accuracy_rfc_grid = round(accuracy_score(y_train_pred_rfc_grid,y_train), 3)
val_accuracy_rfc_grid = round(accuracy_score(y_val_pred_rfc_grid,y_val), 3)
auc_rfc_grid = round(roc_auc_score(y_val_pred_rfc_grid,y_val), 3)

print("The accuracy on train data is ", train_accuracy_rfc_grid)
print("The accuracy on validation data is ", val_accuracy_rfc_grid)
print("The auc on validation data is ", auc_rfc_grid)

"""* Not much difference between train and validation accuracy, that is no overfittiing.
* Achieved highest accuracy of 93% with hyperparameter tuning on Random Forest Classifier.

# **Predictions**
"""

# Loading given test data to predict
test_data = pd.read_csv('/content/drive/MyDrive/Company Assignment/Arya AI/test_set.csv')

test_data.drop('Unnamed: 0', axis=1, inplace=True)

test_data.head()

# standardizing data
X_test = scaler.transform(test_data)

# making datframe from above output arrays
X_test = pd.DataFrame(X_test, columns=train_data.columns[:-1])

# considering top features only
X_test_final = X_test[top_features]

#predictions of class with random forest classifier
predictions = rfc_optimal_model.predict(X_test_final)

# array of class predictions
predictions

# creating column and adding into original test dataset
test_data['Predictions of Y'] = predictions

# dataset with prediction of Y
test_data

test_data.to_csv(r'"C:\Users\amols\Desktop\Predictions.csv"')

!pip reqs C:\Users\amols\Desktop\Companies Info n Assignment

!pip freeze

requirements.txt

!pip install pipreqs

pipreqs ['savepath'] ['C:\Users\amols\Desktop\Companies Info n Assignment\Arya AI\requirement.txt.docx']

!pipreqs .

pipreqs \Users\amols\Desktop

