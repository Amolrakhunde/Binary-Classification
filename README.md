<h1 align="center"> Binary Classification </h1>


![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/rainbow.png)

## ðŸ“‹ Project Files Description
- [Exploratory Data Analysis](https://github.com/Amolrakhunde/Binary-Classification/blob/main/EDA.ipynb)
- [Feature Selection](https://github.com/Amolrakhunde/Binary-Classification/blob/main/EDA.ipynb)
- [Preprocessing](https://github.com/Amolrakhunde/Binary-Classification/blob/main/EDA.ipynb)
- [Training a model to classify as class â€˜0â€™ or class â€˜1â€™](https://github.com/Amolrakhunde/Binary-Classification/blob/main/Model_Performance.ipynb)
- [Predictions](https://github.com/Amolrakhunde/Binary-Classification/blob/main/Predictions.ipynb)


![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/rainbow.png)

##  ðŸ’¾ Data Source

- [Training Dataset](https://github.com/Amolrakhunde/Binary-Classification/blob/main/training_set.csv) - Split into train and validation set in ratio 4:1.
- [Test Dataset](https://github.com/Amolrakhunde/Binary-Classification/blob/main/test_set.csv) - Used for model predictions.


![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/rainbow.png)

## ðŸ“– Binary Classification Model

Dataset have 3614 rows after removing duplicates rows and consists of lot of outliers. So without removing outliers, I performed standardization and used models and techniques which are robust to outliers. I have used Random Forest Classifier for feature selection. Selected top 24 features, which cummulatively gives 90% of cummulatively importance. Then I have used robust to outliers Classification algorithms such Decision Tree Classifier and Random Forest Classifier. Later I performed cross validation and hyperparameter tuning, in order to overcome overfitting and increase performance of model. Then used best model having highest accuracy without overfitting for prediction on test data.

![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/rainbow.png)

##  ðŸ’¾ Performed Steps

1. Importing Libraries & Loading Dataset
2. Data Cleaning
3. Data Processing
4. Exploratory Data Analysis
5. Feature Selection
6. Model Making
7. Cross Validation & Hyperparameter Tuning
8. Comparision of Model
9. Model Predictions

![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/rainbow.png)

## ðŸ”— Credits
#### Amol Rakhunde | Avid Learner | Data Scientist | Machine Learning Enthusiast | Deep Learning Enthusiast

Contact me for Data Science Project Collaborations

[![GitHub](https://img.shields.io/badge/my_portfolio-000?style=for-the-badge&logo=ko-fi&logoColor=white)](https://github.com/Amolrakhunde)
[![LinkedIn](https://img.shields.io/badge/linkedin-0A66C2?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/amol-rakhunde/)
[![Medium](https://img.shields.io/badge/Medium-1DA1F2?style=for-the-badge&logo=medium&logoColor=white)](https://medium.com/@amol_rakhunde)

![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/rainbow.png)

## ðŸ›  Skills
Deep Learning, Machine Learning, SQL, Python, Tableau

![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/rainbow.png)



## ðŸ“œ Feedback

If you have any feedback, please reach out to us at amolsr92@gmail.com


